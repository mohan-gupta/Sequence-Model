{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf57e65",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "* LSTM network rely on a gated cell to track information throughout many time steps.\n",
    "* Maintain a separate cell state from what is outputted.\n",
    "* Use gates to control the flow of information\n",
    "    - **Forget gate** gets rid of irrelevant information.\n",
    "    - **Store** relevant information from curren input.\n",
    "    - Selectively **update** cell state.\n",
    "    - **Output gate** returns a filtered version of the cell state\n",
    "* Backpropogation through time with uninterrupted gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f416a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874046c",
   "metadata": {},
   "source": [
    "# Step by Step LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56079d95",
   "metadata": {},
   "source": [
    "- <font size=3>Step 1: Using forget deciding what information we need to forget and what needs to be kept</font><br>\n",
    "<img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" height=\"720\" width=\"480\"><br>\n",
    "<br>\n",
    "- <font size=3>Step 2: Deciding what new information we will store<br></font>\n",
    "<br>\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" height=\"520\" width=\"440\" align=\"left\">\n",
    "    <img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" height=\"520\" width=\"440\">\n",
    "    <em>First input gate decides which value will be updated.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<em>Update the cell state</em><br> \n",
    "        Then, a tanh layer using which we scales the input gate<br> values by how much we decide to update each state value</em>\n",
    "</p><br>\n",
    "\n",
    "- <font size=3>Step 3: Finally, we calculate the output and new hidden state</font><br>\n",
    "<img src = \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" height=\"720\" width=\"480\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87566f19",
   "metadata": {},
   "source": [
    "### LSTM equations:\n",
    "Input vector:&emsp;&emsp;&emsp;$\\large {X_t} $\n",
    "<br><br>\n",
    "Forget Gate: &ensp;&nbsp;&emsp;&emsp;$\\large f_t = \\sigma(W_{xf} X_t + W_{hf} H_{t-1}) $\n",
    "<br><br>\n",
    "Input Gate:&emsp;&emsp;&emsp;&emsp; $ \\large i_t = \\sigma(W_{xi} X_t + W_{hi} H_{t-1})$\n",
    "<br><br>\n",
    "Candidate: &emsp;&emsp;&emsp;&emsp; $ \\large \\tilde c_t = tanh(W_{xc} X_t + W_{hc} H_{t-1})$\n",
    "<br><br>\n",
    "Udate Cell state: &emsp;&emsp; $ \\large c_t = f_t*c_{t-1} + i_t*\\tilde c_t$\n",
    "<br><br>\n",
    "Output: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $ \\large o_t = \\sigma(W_{xo} X_t + W_{ho} H_{t-1})$\n",
    "<br><br>\n",
    "Update Hidden state: &emsp; $ \\large h_t = o_t * tanh(c_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac7272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.w_xf = nn.Linear(input_size, hidden_size)\n",
    "        self.w_hf = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.w_xi = nn.Linear(input_size, hidden_size)\n",
    "        self.w_hi = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.w_xc = nn.Linear(input_size, hidden_size)\n",
    "        self.w_hc = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.w_xo = nn.Linear(input_size, hidden_size)\n",
    "        self.w_ho = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def init_hidden_cell(self):\n",
    "        #initializing hidden state as a tensor of zeros\n",
    "        # shape -> num_layers, batch_size, hidden_size\n",
    "        hidden = torch.zeros((1,1,hidden_size), dtype=torch.float32)\n",
    "        cell_state = torch.zeros((1,1,hidden_size), dtype=torch.float32)\n",
    "        return hidden, cell_state\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state):   \n",
    "        forget_gate = torch.sigmoid(self.w_xf(x) + self.w_hf(hidden))\n",
    "        \n",
    "        input_gate = torch.sigmoid(self.w_xi(x) + self.w_hi(hidden))\n",
    "        \n",
    "        candidate = torch.tanh(self.w_xc(x) + self.w_hc(hidden))\n",
    "        \n",
    "        cell_state = torch.mul(forget_gate, cell_state) + torch.mul(input_gate, candidate) #pointwise multiplication\n",
    "        \n",
    "        output_gate = torch.sigmoid(self.w_xo(x) + self.w_ho(hidden))\n",
    "        \n",
    "        hidden_state = torch.mul(output_gate, torch.tanh(cell_state))\n",
    "        \n",
    "        return hidden_state, hidden_state[-1], cell_state[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f21bbc",
   "metadata": {},
   "source": [
    "Reference: [colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "[MIT Lecture](https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
